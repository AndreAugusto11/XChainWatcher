{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import requests\n",
    "import threading\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_api_key():\n",
    "    return os.getenv('RONIN_API_KEY')\n",
    "\n",
    "CONNECTION_URL = \"https://api-gateway.skymavis.com/rpc/archive\"\n",
    "OPTIONS = {\n",
    "    'headers': {\n",
    "        'x-api-key': get_api_key()\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_logs(from_block, to_block, topic):\n",
    "    payload = {\n",
    "        \"jsonrpc\": \"2.0\",\n",
    "        \"method\": \"eth_getLogs\",\n",
    "        \"params\": [{\n",
    "            \"fromBlock\": hex(from_block),\n",
    "            \"toBlock\": hex(to_block),\n",
    "            \"address\": \"0xe35d62ebe18413d96ca2a2f7cf215bb21a406b4b\",\n",
    "            \"topics\": topic\n",
    "        }],\n",
    "        \"id\": 1\n",
    "    }\n",
    "\n",
    "    response = requests.post(CONNECTION_URL, headers=OPTIONS['headers'], json=payload)\n",
    "    response_json = response.json()\n",
    "    if 'result' in response_json:\n",
    "        return response_json['result']\n",
    "    else:\n",
    "        print(f\"Error fetching logs for blocks {from_block} to {to_block}: {response_json}\")\n",
    "        return []\n",
    "\n",
    "def save_logs(logs, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(logs, f)\n",
    "\n",
    "def extract_transaction_hashes(logs):\n",
    "    return set(log['transactionHash'] for log in logs)\n",
    "\n",
    "def save_hashes_to_csv(hashes, filename):\n",
    "    with open(filename, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['hash'])\n",
    "        for tx_hash in hashes:\n",
    "            writer.writerow([tx_hash])\n",
    "\n",
    "def main(from_block, to_block, folder_name, topic):\n",
    "    block_range = 500\n",
    "    futures = []\n",
    "    total_logs_count = 0\n",
    "    merged_logs = []\n",
    "\n",
    "    try:\n",
    "\n",
    "        if not os.path.exists(f'./data/{folder_name}/'):\n",
    "            os.makedirs(f'./data/{folder_name}/')\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            for start in range(from_block, to_block, block_range):\n",
    "                end = min(start + block_range - 1, to_block)\n",
    "                future = executor.submit(get_logs, start, end, topic)\n",
    "                futures.append((future, start, end))\n",
    "\n",
    "            for future, start, end in futures:\n",
    "                logs = future.result()\n",
    "                total_logs_count += len(logs)\n",
    "                merged_logs.extend(logs)\n",
    "\n",
    "        merged_logs_count = len(merged_logs)\n",
    "        with open(f'./data/{folder_name}/merged_logs.json', 'w') as f:\n",
    "            json.dump(merged_logs, f)\n",
    "        \n",
    "        print(f\"Merged all log files into ./data/{folder_name}/merged_logs.json\")\n",
    "        print(f\"Total logs fetched: {total_logs_count}\")\n",
    "        print(f\"Total logs in merged file: {merged_logs_count}\")\n",
    "\n",
    "        # Extract transaction hashes and save to CSV\n",
    "        transaction_hashes = extract_transaction_hashes(merged_logs)\n",
    "        save_hashes_to_csv(transaction_hashes, f'./data/{folder_name}/transaction_hashes.csv')\n",
    "\n",
    "        print(f\"Transaction hashes saved to ./data/{folder_name}/transaction_hashes.csv\")\n",
    "        print(f\"Total transaction hashes: {len(transaction_hashes)}\")\n",
    "    except Exception as e:\n",
    "        with open(f'./data/{folder_name}/errors.txt', 'w') as f:\n",
    "            f.write(str(e))\n",
    "        print(f\"Error!! Saved to ./data/{folder_name}/errors.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relevant Block Numbers**\n",
    "\n",
    "663969 # 13 sep\n",
    "\n",
    "9803808 # 1 jan\n",
    "\n",
    "11867416 # 13 Mar\n",
    "\n",
    "13191590 # 28 Apr\n",
    "\n",
    "**Relevant Topics**\n",
    "TokenWithdrew(uint256,address,address,address,uint32,uint256): 0xd56c021eb1befc5273569485864a514b5d80a6192ce1181668ac7c553212558e\n",
    "\n",
    "TokenDeposited(uint256,address,address,uint256): 0x5187d31a2b0e5829ff24ba2d281e6506286752e3d938cbaa86d0202f509ffeb0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged all log files into ./data/logs-01Jan-28Apr-withds/merged_logs.json\n",
      "Total logs fetched: 35413\n",
      "Total logs in merged file: 35413\n",
      "Transaction hashes saved to ./data/logs-01Jan-28Apr-withds/transaction_hashes.csv\n",
      "Total transaction hashes: 35411\n"
     ]
    }
   ],
   "source": [
    "from_block = 9803808 # 1 jan\n",
    "to_block = 13191590 # 28 Apr\n",
    "topics = [\"0xd56c021eb1befc5273569485864a514b5d80a6192ce1181668ac7c553212558e\"]\n",
    "main(from_block, to_block, \"logs-01Jan-28Apr-withds\", topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged all log files into ./data/logs-01Jan-28Apr-deps/merged_logs.json\n",
      "Total logs fetched: 43990\n",
      "Total logs in merged file: 43990\n",
      "Transaction hashes saved to ./data/logs-01Jan-28Apr-deps/transaction_hashes.csv\n",
      "Total transaction hashes: 39691\n"
     ]
    }
   ],
   "source": [
    "from_block = 9803808 # 1 jan\n",
    "to_block = 13191590 # 28 Apr\n",
    "topics = [\"0x5187d31a2b0e5829ff24ba2d281e6506286752e3d938cbaa86d0202f509ffeb0\"]\n",
    "main(from_block, to_block, \"logs-01Jan-28Apr-deps\", topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged all log files into ./data/logs-13Sep-31Dec/merged_logs.json\n",
      "Total logs fetched: 0\n",
      "Total logs in merged file: 1028950\n",
      "Transaction hashes saved to ./data/logs-13Sep-31Dec/transaction_hashes.csv\n",
      "Total transaction hashes: 1028950\n"
     ]
    }
   ],
   "source": [
    "topics = [\"0xd56c021eb1befc5273569485864a514b5d80a6192ce1181668ac7c553212558e\"]\n",
    "main(663969, 9803807, \"logs-13Sep-31Dec\", topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Transaction Receipts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "RATE_LIMIT = 20  # requests per second\n",
    "REQUEST_INTERVAL = 1 / RATE_LIMIT\n",
    "\n",
    "def fetch_receipt(tx_hash, folder_name):\n",
    "    data = {\n",
    "        \"id\": 1,\n",
    "        \"jsonrpc\": \"2.0\",\n",
    "        \"method\": \"eth_getTransactionReceipt\",\n",
    "        \"params\": [tx_hash]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(CONNECTION_URL, headers=OPTIONS['headers'], json=data)\n",
    "        response_json = response.json()\n",
    "        if 'result' in response_json:\n",
    "            return response_json['result']\n",
    "        else:\n",
    "            print(f\"Error fetching logs for blocks {from_block} to {to_block}: {response_json}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        with open(f\"./data/{folder_name}/errors.txt\", \"a\") as error_file:\n",
    "            error_file.write(f\"Error retrieving transaction: {tx_hash}\\n\")\n",
    "\n",
    "def process_hashes(hashes, folder_name):\n",
    "    receipts = []\n",
    "    total_hashes = len(hashes)\n",
    "    progress_interval = max(1, total_hashes // 100)  # Update progress every 1%\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=RATE_LIMIT) as executor:\n",
    "        futures = {executor.submit(fetch_receipt, tx_hash, folder_name): tx_hash for tx_hash in hashes}\n",
    "        \n",
    "        for idx, future in enumerate(as_completed(futures), 1):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                receipts.append(result)\n",
    "\n",
    "                if idx % progress_interval == 0 or idx == total_hashes:\n",
    "                    progress_percentage = (idx / total_hashes) * 100\n",
    "                    print(f\"Progress: {progress_percentage:.2f}% ({idx}/{total_hashes})\")\n",
    "        \n",
    "                time.sleep(REQUEST_INTERVAL)\n",
    "            except Exception as e:\n",
    "                with open(f\"./data/{folder_name}/errors.txt\", \"a\") as error_file:\n",
    "                    error_file.write(f\"Error retrieving transaction: {futures[future]}\\n\")\n",
    "    return receipts\n",
    "\n",
    "def retrieve_receipts(folder_name):\n",
    "    # Read hashes from CSV\n",
    "    hashes_df = pd.read_csv(f'./data/{folder_name}/transaction_hashes.csv')\n",
    "    hashes = hashes_df['hash'].drop_duplicates().tolist()\n",
    "\n",
    "    # Fetch receipts\n",
    "    receipts = process_hashes(hashes, folder_name)\n",
    "\n",
    "    # Save receipts to JSON\n",
    "    with open(f'./data/{folder_name}/tx_receipts.json', 'w') as f:\n",
    "        json.dump(receipts, f)\n",
    "\n",
    "    # Print the count of processed receipts\n",
    "    print(f\"Total receipts fetched: {len(receipts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1.00% (396/39691)\n",
      "Progress: 2.00% (792/39691)\n",
      "Progress: 2.99% (1188/39691)\n",
      "Progress: 3.99% (1584/39691)\n",
      "Progress: 4.99% (1980/39691)\n",
      "Progress: 5.99% (2376/39691)\n",
      "Progress: 6.98% (2772/39691)\n",
      "Progress: 7.98% (3168/39691)\n",
      "Progress: 8.98% (3564/39691)\n",
      "Progress: 9.98% (3960/39691)\n",
      "Progress: 10.97% (4356/39691)\n",
      "Progress: 11.97% (4752/39691)\n",
      "Progress: 12.97% (5148/39691)\n",
      "Progress: 13.97% (5544/39691)\n",
      "Progress: 14.97% (5940/39691)\n",
      "Progress: 15.96% (6336/39691)\n",
      "Progress: 16.96% (6732/39691)\n",
      "Progress: 17.96% (7128/39691)\n",
      "Progress: 18.96% (7524/39691)\n",
      "Progress: 19.95% (7920/39691)\n",
      "Progress: 20.95% (8316/39691)\n",
      "Progress: 21.95% (8712/39691)\n",
      "Progress: 22.95% (9108/39691)\n",
      "Progress: 23.94% (9504/39691)\n",
      "Progress: 24.94% (9900/39691)\n",
      "Progress: 25.94% (10296/39691)\n",
      "Progress: 26.94% (10692/39691)\n",
      "Progress: 27.94% (11088/39691)\n",
      "Progress: 28.93% (11484/39691)\n",
      "Progress: 29.93% (11880/39691)\n",
      "Progress: 30.93% (12276/39691)\n",
      "Progress: 31.93% (12672/39691)\n",
      "Progress: 32.92% (13068/39691)\n",
      "Progress: 33.92% (13464/39691)\n",
      "Progress: 34.92% (13860/39691)\n",
      "Progress: 35.92% (14256/39691)\n",
      "Progress: 36.92% (14652/39691)\n",
      "Progress: 37.91% (15048/39691)\n",
      "Progress: 38.91% (15444/39691)\n",
      "Progress: 39.91% (15840/39691)\n",
      "Progress: 40.91% (16236/39691)\n",
      "Progress: 41.90% (16632/39691)\n",
      "Progress: 42.90% (17028/39691)\n",
      "Progress: 43.90% (17424/39691)\n",
      "Progress: 44.90% (17820/39691)\n",
      "Progress: 45.89% (18216/39691)\n",
      "Progress: 46.89% (18612/39691)\n",
      "Progress: 47.89% (19008/39691)\n",
      "Progress: 48.89% (19404/39691)\n",
      "Progress: 49.89% (19800/39691)\n",
      "Progress: 50.88% (20196/39691)\n",
      "Progress: 51.88% (20592/39691)\n",
      "Progress: 52.88% (20988/39691)\n",
      "Progress: 53.88% (21384/39691)\n",
      "Progress: 54.87% (21780/39691)\n",
      "Progress: 55.87% (22176/39691)\n",
      "Progress: 56.87% (22572/39691)\n",
      "Progress: 57.87% (22968/39691)\n",
      "Progress: 58.86% (23364/39691)\n",
      "Progress: 59.86% (23760/39691)\n",
      "Progress: 60.86% (24156/39691)\n",
      "Progress: 61.86% (24552/39691)\n",
      "Progress: 62.86% (24948/39691)\n",
      "Progress: 63.85% (25344/39691)\n",
      "Progress: 64.85% (25740/39691)\n",
      "Progress: 65.85% (26136/39691)\n",
      "Progress: 66.85% (26532/39691)\n",
      "Progress: 67.84% (26928/39691)\n",
      "Progress: 68.84% (27324/39691)\n",
      "Progress: 69.84% (27720/39691)\n",
      "Progress: 70.84% (28116/39691)\n",
      "Progress: 71.83% (28512/39691)\n",
      "Progress: 72.83% (28908/39691)\n",
      "Progress: 73.83% (29304/39691)\n",
      "Progress: 74.83% (29700/39691)\n",
      "Progress: 75.83% (30096/39691)\n",
      "Progress: 76.82% (30492/39691)\n",
      "Progress: 77.82% (30888/39691)\n",
      "Progress: 78.82% (31284/39691)\n",
      "Progress: 79.82% (31680/39691)\n",
      "Progress: 80.81% (32076/39691)\n",
      "Progress: 81.81% (32472/39691)\n",
      "Progress: 82.81% (32868/39691)\n",
      "Progress: 83.81% (33264/39691)\n",
      "Progress: 84.81% (33660/39691)\n",
      "Progress: 85.80% (34056/39691)\n",
      "Progress: 86.80% (34452/39691)\n",
      "Progress: 87.80% (34848/39691)\n",
      "Progress: 88.80% (35244/39691)\n",
      "Progress: 89.79% (35640/39691)\n",
      "Progress: 90.79% (36036/39691)\n",
      "Progress: 91.79% (36432/39691)\n",
      "Progress: 92.79% (36828/39691)\n",
      "Progress: 93.78% (37224/39691)\n",
      "Progress: 94.78% (37620/39691)\n",
      "Progress: 95.78% (38016/39691)\n",
      "Progress: 96.78% (38412/39691)\n",
      "Progress: 97.78% (38808/39691)\n",
      "Progress: 98.77% (39204/39691)\n",
      "Progress: 99.77% (39600/39691)\n",
      "Progress: 100.00% (39691/39691)\n",
      "Total receipts fetched: 39691\n"
     ]
    }
   ],
   "source": [
    "retrieve_receipts(\"logs-01Jan-28Apr-deps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total receipts fetched: 0\n"
     ]
    }
   ],
   "source": [
    "retrieve_receipts(\"logs-01Jan-28Apr-withds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1.00% (10289/1028951)\n",
      "Progress: 2.00% (20578/1028951)\n",
      "Progress: 3.00% (30867/1028951)\n",
      "Progress: 4.00% (41156/1028951)\n",
      "Progress: 5.00% (51445/1028951)\n",
      "Progress: 6.00% (61734/1028951)\n",
      "Progress: 7.00% (72023/1028951)\n",
      "Progress: 8.00% (82312/1028951)\n",
      "Progress: 9.00% (92601/1028951)\n",
      "Progress: 10.00% (102890/1028951)\n",
      "Progress: 11.00% (113179/1028951)\n",
      "Progress: 12.00% (123468/1028951)\n",
      "Progress: 13.00% (133757/1028951)\n",
      "Progress: 14.00% (144046/1028951)\n",
      "Progress: 15.00% (154335/1028951)\n",
      "Progress: 16.00% (164624/1028951)\n",
      "Progress: 17.00% (174913/1028951)\n",
      "Progress: 18.00% (185202/1028951)\n",
      "Progress: 19.00% (195491/1028951)\n",
      "Progress: 20.00% (205780/1028951)\n",
      "Progress: 21.00% (216069/1028951)\n",
      "Progress: 22.00% (226358/1028951)\n",
      "Progress: 23.00% (236647/1028951)\n",
      "Progress: 24.00% (246936/1028951)\n",
      "Progress: 25.00% (257225/1028951)\n",
      "Progress: 26.00% (267514/1028951)\n",
      "Progress: 27.00% (277803/1028951)\n",
      "Progress: 28.00% (288092/1028951)\n",
      "Progress: 29.00% (298381/1028951)\n",
      "Progress: 30.00% (308670/1028951)\n",
      "Progress: 31.00% (318959/1028951)\n",
      "Progress: 32.00% (329248/1028951)\n",
      "Progress: 33.00% (339537/1028951)\n",
      "Progress: 34.00% (349826/1028951)\n",
      "Progress: 35.00% (360115/1028951)\n",
      "Progress: 36.00% (370404/1028951)\n",
      "Progress: 37.00% (380693/1028951)\n",
      "Progress: 38.00% (390982/1028951)\n",
      "Progress: 39.00% (401271/1028951)\n",
      "Progress: 40.00% (411560/1028951)\n",
      "Progress: 41.00% (421849/1028951)\n",
      "Progress: 42.00% (432138/1028951)\n",
      "Progress: 43.00% (442427/1028951)\n",
      "Progress: 44.00% (452716/1028951)\n",
      "Progress: 45.00% (463005/1028951)\n",
      "Progress: 46.00% (473294/1028951)\n",
      "Progress: 47.00% (483583/1028951)\n",
      "Progress: 48.00% (493872/1028951)\n",
      "Progress: 49.00% (504161/1028951)\n",
      "Progress: 50.00% (514450/1028951)\n",
      "Progress: 51.00% (524739/1028951)\n",
      "Progress: 52.00% (535028/1028951)\n",
      "Progress: 53.00% (545317/1028951)\n",
      "Progress: 54.00% (555606/1028951)\n",
      "Progress: 55.00% (565895/1028951)\n",
      "Progress: 56.00% (576184/1028951)\n",
      "Progress: 57.00% (586473/1028951)\n",
      "Progress: 58.00% (596762/1028951)\n",
      "Progress: 59.00% (607051/1028951)\n",
      "Progress: 60.00% (617340/1028951)\n",
      "Progress: 61.00% (627629/1028951)\n",
      "Progress: 62.00% (637918/1028951)\n",
      "Progress: 63.00% (648207/1028951)\n",
      "Progress: 64.00% (658496/1028951)\n",
      "Progress: 65.00% (668785/1028951)\n",
      "Progress: 66.00% (679074/1028951)\n",
      "Progress: 67.00% (689363/1028951)\n",
      "Progress: 68.00% (699652/1028951)\n",
      "Progress: 69.00% (709941/1028951)\n",
      "Progress: 70.00% (720230/1028951)\n",
      "Progress: 71.00% (730519/1028951)\n",
      "Progress: 72.00% (740808/1028951)\n",
      "Progress: 73.00% (751097/1028951)\n",
      "Progress: 74.00% (761386/1028951)\n",
      "Progress: 75.00% (771675/1028951)\n",
      "Progress: 76.00% (781964/1028951)\n",
      "Progress: 77.00% (792253/1028951)\n",
      "Progress: 78.00% (802542/1028951)\n",
      "Progress: 79.00% (812831/1028951)\n",
      "Progress: 80.00% (823120/1028951)\n",
      "Progress: 81.00% (833409/1028951)\n",
      "Progress: 82.00% (843698/1028951)\n",
      "Progress: 83.00% (853987/1028951)\n",
      "Progress: 84.00% (864276/1028951)\n",
      "Progress: 85.00% (874565/1028951)\n",
      "Progress: 86.00% (884854/1028951)\n",
      "Progress: 87.00% (895143/1028951)\n",
      "Progress: 88.00% (905432/1028951)\n",
      "Progress: 89.00% (915721/1028951)\n",
      "Progress: 90.00% (926010/1028951)\n",
      "Progress: 91.00% (936299/1028951)\n",
      "Progress: 92.00% (946588/1028951)\n",
      "Progress: 93.00% (956877/1028951)\n",
      "Progress: 94.00% (967166/1028951)\n",
      "Progress: 95.00% (977455/1028951)\n",
      "Progress: 96.00% (987744/1028951)\n",
      "Progress: 97.00% (998033/1028951)\n",
      "Progress: 98.00% (1008322/1028951)\n",
      "Progress: 99.00% (1018611/1028951)\n",
      "Progress: 100.00% (1028900/1028951)\n",
      "Progress: 100.00% (1028951/1028951)\n",
      "Total receipts fetched: 1028867\n"
     ]
    }
   ],
   "source": [
    "retrieve_receipts('logs-13Sep-31Dec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_failed_hashes(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        hashes = [line.strip() for line in f]\n",
    "    return hashes\n",
    "\n",
    "def retrieve_failed_receipts(folder_name):\n",
    "    errors_file = f'./data/{folder_name}/errors.txt'\n",
    "    if not os.path.exists(errors_file):\n",
    "        print(f\"Error file {errors_file} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Load failed transaction hashes from errors.txt\n",
    "    failed_hashes = load_failed_hashes(f'./data/{folder_name}/errors.txt')\n",
    "\n",
    "    # Fetch receipts for failed hashes\n",
    "    failed_receipts = process_hashes(failed_hashes, folder_name)\n",
    "\n",
    "    # Save failed receipts to JSON\n",
    "    with open(f'./data/{folder_name}/tx_receipts_2.json', 'w') as f:\n",
    "        json.dump(failed_receipts, f)\n",
    "\n",
    "    # Print the count of processed failed receipts\n",
    "    print(f\"Total failed receipts fetched: {len(failed_receipts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 33.33% (1/3)\n",
      "Progress: 66.67% (2/3)\n",
      "Progress: 100.00% (3/3)\n",
      "Total failed receipts fetched: 3\n"
     ]
    }
   ],
   "source": [
    "retrieve_failed_receipts('logs-01Jan-28Apr-deps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_failed_receipts('logs-01Jan-28Apr-withds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.60% (1/166)\n",
      "Progress: 1.20% (2/166)\n",
      "Progress: 1.81% (3/166)\n",
      "Progress: 2.41% (4/166)\n",
      "Progress: 3.01% (5/166)\n",
      "Progress: 3.61% (6/166)\n",
      "Progress: 4.22% (7/166)\n",
      "Progress: 4.82% (8/166)\n",
      "Progress: 5.42% (9/166)\n",
      "Progress: 6.02% (10/166)\n",
      "Progress: 6.63% (11/166)\n",
      "Progress: 7.23% (12/166)\n",
      "Progress: 7.83% (13/166)\n",
      "Progress: 8.43% (14/166)\n",
      "Progress: 9.04% (15/166)\n",
      "Progress: 9.64% (16/166)\n",
      "Progress: 10.24% (17/166)\n",
      "Progress: 10.84% (18/166)\n",
      "Progress: 11.45% (19/166)\n",
      "Progress: 12.05% (20/166)\n",
      "Progress: 12.65% (21/166)\n",
      "Progress: 13.25% (22/166)\n",
      "Progress: 13.86% (23/166)\n",
      "Progress: 14.46% (24/166)\n",
      "Progress: 15.06% (25/166)\n",
      "Progress: 15.66% (26/166)\n",
      "Progress: 16.27% (27/166)\n",
      "Progress: 16.87% (28/166)\n",
      "Progress: 17.47% (29/166)\n",
      "Progress: 18.07% (30/166)\n",
      "Progress: 18.67% (31/166)\n",
      "Progress: 19.28% (32/166)\n",
      "Progress: 19.88% (33/166)\n",
      "Progress: 20.48% (34/166)\n",
      "Progress: 21.08% (35/166)\n",
      "Progress: 21.69% (36/166)\n",
      "Progress: 22.29% (37/166)\n",
      "Progress: 22.89% (38/166)\n",
      "Progress: 23.49% (39/166)\n",
      "Progress: 24.10% (40/166)\n",
      "Progress: 24.70% (41/166)\n",
      "Progress: 25.30% (42/166)\n",
      "Progress: 25.90% (43/166)\n",
      "Progress: 26.51% (44/166)\n",
      "Progress: 27.11% (45/166)\n",
      "Progress: 27.71% (46/166)\n",
      "Progress: 28.31% (47/166)\n",
      "Progress: 28.92% (48/166)\n",
      "Progress: 29.52% (49/166)\n",
      "Progress: 30.12% (50/166)\n",
      "Progress: 30.72% (51/166)\n",
      "Progress: 31.33% (52/166)\n",
      "Progress: 31.93% (53/166)\n",
      "Progress: 32.53% (54/166)\n",
      "Progress: 33.13% (55/166)\n",
      "Progress: 33.73% (56/166)\n",
      "Progress: 34.34% (57/166)\n",
      "Progress: 34.94% (58/166)\n",
      "Progress: 35.54% (59/166)\n",
      "Progress: 36.14% (60/166)\n",
      "Progress: 36.75% (61/166)\n",
      "Progress: 37.35% (62/166)\n",
      "Progress: 37.95% (63/166)\n",
      "Progress: 38.55% (64/166)\n",
      "Progress: 39.16% (65/166)\n",
      "Progress: 39.76% (66/166)\n",
      "Progress: 40.36% (67/166)\n",
      "Progress: 40.96% (68/166)\n",
      "Progress: 41.57% (69/166)\n",
      "Progress: 42.17% (70/166)\n",
      "Progress: 42.77% (71/166)\n",
      "Progress: 43.37% (72/166)\n",
      "Progress: 43.98% (73/166)\n",
      "Progress: 44.58% (74/166)\n",
      "Progress: 45.18% (75/166)\n",
      "Progress: 45.78% (76/166)\n",
      "Progress: 46.39% (77/166)\n",
      "Progress: 46.99% (78/166)\n",
      "Progress: 47.59% (79/166)\n",
      "Progress: 48.19% (80/166)\n",
      "Progress: 48.80% (81/166)\n",
      "Progress: 49.40% (82/166)\n",
      "Progress: 50.00% (83/166)\n",
      "Progress: 50.60% (84/166)\n",
      "Progress: 51.20% (85/166)\n",
      "Progress: 51.81% (86/166)\n",
      "Progress: 52.41% (87/166)\n",
      "Progress: 53.01% (88/166)\n",
      "Progress: 53.61% (89/166)\n",
      "Progress: 54.22% (90/166)\n",
      "Progress: 54.82% (91/166)\n",
      "Progress: 55.42% (92/166)\n",
      "Progress: 56.02% (93/166)\n",
      "Progress: 56.63% (94/166)\n",
      "Progress: 57.23% (95/166)\n",
      "Progress: 57.83% (96/166)\n",
      "Progress: 58.43% (97/166)\n",
      "Progress: 59.04% (98/166)\n",
      "Progress: 59.64% (99/166)\n",
      "Progress: 60.24% (100/166)\n",
      "Progress: 60.84% (101/166)\n",
      "Progress: 61.45% (102/166)\n",
      "Progress: 62.05% (103/166)\n",
      "Progress: 62.65% (104/166)\n",
      "Progress: 63.25% (105/166)\n",
      "Progress: 63.86% (106/166)\n",
      "Progress: 64.46% (107/166)\n",
      "Progress: 65.06% (108/166)\n",
      "Progress: 65.66% (109/166)\n",
      "Progress: 66.27% (110/166)\n",
      "Progress: 66.87% (111/166)\n",
      "Progress: 67.47% (112/166)\n",
      "Progress: 68.07% (113/166)\n",
      "Progress: 68.67% (114/166)\n",
      "Progress: 69.28% (115/166)\n",
      "Progress: 69.88% (116/166)\n",
      "Progress: 70.48% (117/166)\n",
      "Progress: 71.08% (118/166)\n",
      "Progress: 71.69% (119/166)\n",
      "Progress: 72.29% (120/166)\n",
      "Progress: 72.89% (121/166)\n",
      "Progress: 73.49% (122/166)\n",
      "Progress: 74.10% (123/166)\n",
      "Progress: 74.70% (124/166)\n",
      "Progress: 75.30% (125/166)\n",
      "Progress: 75.90% (126/166)\n",
      "Progress: 76.51% (127/166)\n",
      "Progress: 77.11% (128/166)\n",
      "Progress: 77.71% (129/166)\n",
      "Progress: 78.31% (130/166)\n",
      "Progress: 78.92% (131/166)\n",
      "Progress: 79.52% (132/166)\n",
      "Progress: 80.12% (133/166)\n",
      "Progress: 80.72% (134/166)\n",
      "Progress: 81.33% (135/166)\n",
      "Progress: 81.93% (136/166)\n",
      "Progress: 82.53% (137/166)\n",
      "Progress: 83.13% (138/166)\n",
      "Progress: 83.73% (139/166)\n",
      "Progress: 84.34% (140/166)\n",
      "Progress: 84.94% (141/166)\n",
      "Progress: 85.54% (142/166)\n",
      "Progress: 86.14% (143/166)\n",
      "Progress: 86.75% (144/166)\n",
      "Progress: 87.35% (145/166)\n",
      "Progress: 87.95% (146/166)\n",
      "Progress: 88.55% (147/166)\n",
      "Progress: 89.16% (148/166)\n",
      "Progress: 89.76% (149/166)\n",
      "Progress: 90.36% (150/166)\n",
      "Progress: 90.96% (151/166)\n",
      "Progress: 91.57% (152/166)\n",
      "Progress: 92.17% (153/166)\n",
      "Progress: 92.77% (154/166)\n",
      "Progress: 93.37% (155/166)\n",
      "Progress: 93.98% (156/166)\n",
      "Progress: 94.58% (157/166)\n",
      "Progress: 95.18% (158/166)\n",
      "Progress: 95.78% (159/166)\n",
      "Progress: 96.39% (160/166)\n",
      "Progress: 96.99% (161/166)\n",
      "Progress: 97.59% (162/166)\n",
      "Progress: 98.19% (163/166)\n",
      "Progress: 98.80% (164/166)\n",
      "Progress: 99.40% (165/166)\n",
      "Progress: 100.00% (166/166)\n",
      "Total failed receipts fetched: 166\n"
     ]
    }
   ],
   "source": [
    "retrieve_failed_receipts('logs-13Sep-31Dec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second interval, we need to merge the json files with transaction receipts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def merge_json_files(folder_name):\n",
    "    file1 = f'./data/{folder_name}/tx_receipts.json'\n",
    "    file2 = f'./data/{folder_name}/tx_receipts_2.json'\n",
    "    output_file = f'./data/{folder_name}/tx_receipts.json'\n",
    "    \n",
    "    errors_file = f'./data/{folder_name}/errors.txt'\n",
    "    if not os.path.exists(errors_file):\n",
    "        print(f\"Error file {errors_file} does not exist.\")\n",
    "        return\n",
    "    \n",
    "    with open(file1, 'r') as f1, open(file2, 'r') as f2:\n",
    "        json1 = json.load(f1)\n",
    "        json2 = json.load(f2)\n",
    "\n",
    "    merged_json = json1 + json2\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(merged_json, f)\n",
    "\n",
    "    os.remove(file2)\n",
    "\n",
    "    count1 = len(json1)\n",
    "    count2 = len(json2)\n",
    "    count_merged = len(merged_json)\n",
    "\n",
    "    print(f\"Number of transaction receipts in tx_receipts: {count1}\")\n",
    "    print(f\"Number of transaction receipts in tx_receipts_2: {count2}\")\n",
    "    print(f\"Number of transaction receipts in the merged file: {count_merged}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transaction receipts in tx_receipts: 39688\n",
      "Number of transaction receipts in tx_receipts_2: 3\n",
      "Number of transaction receipts in the merged file: 39691\n"
     ]
    }
   ],
   "source": [
    "merge_json_files(\"logs-01Jan-28Apr-deps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error file ./data/logs-01Jan-28Apr-withds/errors.txt does not exist.\n"
     ]
    }
   ],
   "source": [
    "merge_json_files(\"logs-01Jan-28Apr-withds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transaction receipts in ./data/logs-13Sep-31Dec/tx_receipts.json: 1028867\n",
      "Number of transaction receipts in ./data/logs-13Sep-31Dec/tx_receipts_2.json: 166\n",
      "Number of transaction receipts in the merged file: 1029033\n"
     ]
    }
   ],
   "source": [
    "merge_json_files(\"logs-13Sep-31Dec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates from json file with result field of receipts\n",
    "def remove_duplicates(folder_name):\n",
    "    file = f'./data/{folder_name}/tx_receipts.json'\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        receipts = json.load(f)\n",
    "\n",
    "    unique_receipts = []\n",
    "    unique_hashes = set()\n",
    "\n",
    "    for receipt in receipts:\n",
    "        tx_hash = receipt.get('transactionHash')\n",
    "        if tx_hash not in unique_hashes:\n",
    "            unique_hashes.add(tx_hash)\n",
    "            unique_receipts.append(receipt)\n",
    "\n",
    "    # Save unique receipts to a new file\n",
    "    unique_output_file = f'./data/{folder_name}/unique_tx_receipts.json'\n",
    "    with open(unique_output_file, 'w') as f:\n",
    "        json.dump(unique_receipts, f)\n",
    "\n",
    "    print(f\"Unique receipts saved to {unique_output_file}\")\n",
    "\n",
    "    print(f\"Removed {len(receipts) - len(unique_receipts)} duplicate receipts\")\n",
    "    print(f\"Total receipts: {len(receipts)}\")\n",
    "    print(f\"Total unique receipts: {len(unique_receipts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique receipts saved to ./data/logs-01Jan-28Apr-deps/tx_receipts.json\n",
      "Removed 0 duplicate receipts\n",
      "Total receipts: 39691\n",
      "Total unique receipts: 39691\n"
     ]
    }
   ],
   "source": [
    "remove_duplicates(\"logs-01Jan-28Apr-deps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique receipts saved to ./data/logs-01Jan-28Apr-withds/tx_receipts.json\n",
      "Removed 0 duplicate receipts\n",
      "Total receipts: 35411\n",
      "Total unique receipts: 35411\n"
     ]
    }
   ],
   "source": [
    "remove_duplicates( \"logs-01Jan-28Apr-withds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique receipts saved to ./data/logs-13Sep-31Dec/unique_tx_receipts.json\n",
      "Removed 82 duplicate receipts\n",
      "Total receipts: 1029032\n",
      "Total unique receipts: 1028950\n"
     ]
    }
   ],
   "source": [
    "remove_duplicates(\"logs-13Sep-31Dec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import requests\n",
    "\n",
    "def get_block_data(block_number, errors_file):\n",
    "    payload = {\n",
    "        \"id\": 1,\n",
    "        \"jsonrpc\": \"2.0\",\n",
    "        \"params\": [],\n",
    "        \"method\": \"eth_getBlockByNumber\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        payload['params'] = [block_number]\n",
    "        response = requests.post(CONNECTION_URL, headers=OPTIONS['headers'], json=payload)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            block = response.json()[\"result\"]\n",
    "            number = int(block[\"number\"], 16)\n",
    "            timestamp = int(block[\"timestamp\"], 16)\n",
    "            transactions = len(block[\"transactions\"])\n",
    "            return f\"{number},{transactions},{timestamp}\\n\"\n",
    "        else:\n",
    "            with open(errors_file, \"a\") as error_file:\n",
    "                error_file.write(f\"Error code: {block_number}\\n\")\n",
    "    except Exception as e:\n",
    "        with open(errors_file, \"a\") as error_file:\n",
    "            error_file.write(f\"Error retrieving block: {block_number}; {e}\\n\")\n",
    "\n",
    "def get_blocks_data(folder_name):\n",
    "    input_file = f'./data/{folder_name}/tx_receipts.json'\n",
    "    output_file = f'./data/{folder_name}/blocks.csv'\n",
    "    errors_file = f'./data/{folder_name}/errors.txt'\n",
    "\n",
    "    with open(input_file, 'r') as file:\n",
    "        tx_receipts = json.load(file)\n",
    "\n",
    "    print(f\"Extracting block number and Unix timestamp from {len(tx_receipts)} transaction receipts...\")\n",
    "\n",
    "    block_numbers = [tx[\"blockNumber\"] for tx in tx_receipts]\n",
    "\n",
    "    print(f\"Extracted {len(block_numbers)} block numbers...\")\n",
    "\n",
    "    with open(output_file, \"a\") as blocks_file:\n",
    "        blocks_file.write(\"block_number,transactions,timestamp\\n\")\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            # Submit tasks for each block in the range\n",
    "            futures = {executor.submit(get_block_data, block_number, errors_file): block_number for block_number in block_numbers}\n",
    "\n",
    "            # Process the completed tasks and write to the file\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                block_number = futures[future]\n",
    "                try:\n",
    "                    blocks_data = future.result()\n",
    "                    blocks_file.write(blocks_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing block {block_number}: {e}\")\n",
    "\n",
    "    print(f'Extracted block number and Unix timestamp to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting block number and Unix timestamp from 39691 transaction receipts...\n",
      "Extracted 39691 block numbers...\n",
      "Extracted block number and Unix timestamp to ./data/logs-01Jan-28Apr-deps/blocks.csv\n"
     ]
    }
   ],
   "source": [
    "get_blocks_data('logs-01Jan-28Apr-deps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting block number and Unix timestamp from 5022 transaction receipts...\n",
      "Extracted 5022 block numbers...\n",
      "Extracted block number and Unix timestamp to ./data/logs-13Mar-28Apr/blocks.csv\n"
     ]
    }
   ],
   "source": [
    "get_blocks_data('logs-01Jan-28Apr-withds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting block number and Unix timestamp from 1028950 transaction receipts...\n",
      "Extracted 1028950 block numbers...\n",
      "Error processing block 0x865a69: write() argument must be str, not None\n",
      "Error processing block 0x4b70e0: write() argument must be str, not None\n",
      "Error processing block 0x3d8f6c: write() argument must be str, not None\n",
      "Error processing block 0x583132: write() argument must be str, not None\n",
      "Error processing block 0x543c2a: write() argument must be str, not None\n",
      "Error processing block 0x61fc78: write() argument must be str, not None\n",
      "Error processing block 0x553c31: write() argument must be str, not None\n",
      "Error processing block 0x55c190: write() argument must be str, not None\n",
      "Error processing block 0x4cf8aa: write() argument must be str, not None\n",
      "Error processing block 0x6504c8: write() argument must be str, not None\n",
      "Error processing block 0x582272: write() argument must be str, not None\n",
      "Error processing block 0x53e1c1: write() argument must be str, not None\n",
      "Error processing block 0x5662a8: write() argument must be str, not None\n",
      "Error processing block 0x75d411: write() argument must be str, not None\n",
      "Error processing block 0x44cfd7: write() argument must be str, not None\n",
      "Error processing block 0x49dfb8: write() argument must be str, not None\n",
      "Error processing block 0x84de98: write() argument must be str, not None\n",
      "Error processing block 0x507fba: write() argument must be str, not None\n",
      "Error processing block 0x48eee4: write() argument must be str, not None\n",
      "Error processing block 0x799b72: write() argument must be str, not None\n",
      "Error processing block 0x4e7d81: write() argument must be str, not None\n",
      "Error processing block 0x39b803: write() argument must be str, not None\n",
      "Error processing block 0x589472: write() argument must be str, not None\n",
      "Error processing block 0x52d8d1: write() argument must be str, not None\n",
      "Error processing block 0x483e36: write() argument must be str, not None\n",
      "Error processing block 0x734061: write() argument must be str, not None\n",
      "Error processing block 0x506bf1: write() argument must be str, not None\n",
      "Error processing block 0x584769: write() argument must be str, not None\n",
      "Error processing block 0x4b8d38: write() argument must be str, not None\n",
      "Error processing block 0x6aaf99: write() argument must be str, not None\n",
      "Error processing block 0x50913a: write() argument must be str, not None\n",
      "Error processing block 0x9276b1: write() argument must be str, not None\n"
     ]
    }
   ],
   "source": [
    "get_blocks_data('logs-13Sep-31Dec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_missing_blocks(folder_name):\n",
    "    blocks_file = f'./data/{folder_name}/blocks.csv'\n",
    "    receipts_file = f'./data/{folder_name}/tx_receipts.json'\n",
    "    errors_file = f'./data/{folder_name}/errors_2.txt'\n",
    "\n",
    "    blocks_data = pd.read_csv(blocks_file)\n",
    "\n",
    "    with open(receipts_file, 'r') as file:\n",
    "        tx_receipts = json.load(file)\n",
    "\n",
    "    block_numbers = [tx[\"blockNumber\"] for tx in tx_receipts]\n",
    "    \n",
    "    print(\"Loaded block numbers and block data...\")\n",
    "\n",
    "    print(len(block_numbers))\n",
    "    print(len(blocks_data['block_number']))\n",
    "    \n",
    "    missing_blocks = set(block_numbers) - set(hex(block) for block in blocks_data['block_number'])\n",
    "\n",
    "    print(f\"Total missing blocks: {len(missing_blocks)}\")\n",
    "\n",
    "    for missing_block in missing_blocks:\n",
    "        line = get_block_data(missing_block, errors_file)\n",
    "        with open(blocks_file, 'a') as blocks_file:\n",
    "            blocks_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded block numbers and block data...\n",
      "39691\n",
      "39691\n",
      "Total missing blocks: 0\n"
     ]
    }
   ],
   "source": [
    "retrieve_missing_blocks('logs-01Jan-28Apr-deps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded block numbers and block data...\n",
      "5022\n",
      "5022\n",
      "Total missing blocks: 0\n"
     ]
    }
   ],
   "source": [
    "retrieve_missing_blocks('logs-01Jan-28Apr-withds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded block numbers and block data...\n",
      "1028950\n",
      "1028786\n",
      "Total missing blocks: 64\n"
     ]
    }
   ],
   "source": [
    "retrieve_missing_blocks('logs-13Sep-31Dec')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
